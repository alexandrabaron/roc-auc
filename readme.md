# üìö Introduction aux courbes ROC

Dans ce document, nous allons comprendre **ce qu‚Äôest une courbe ROC**, **comment elle est construite**, et **comment l‚Äôinterpr√©ter**.  
Nous allons utiliser des **exemples visuels** simples, issus d‚Äôun dataset artificiel, pour rendre les concepts plus concrets.

---

# 1. Cr√©ation d‚Äôun dataset simple

![Dataset d'entra√Ænement](img/dataset_entra√Ænement.png)

Pour commencer, nous avons g√©n√©r√© un jeu de donn√©es simple :  
- Chaque point est caract√©ris√© par une seule variable \(X\).
- Les √©tiquettes \(y\) prennent la valeur **0** ou **1**.
- Les couleurs indiquent la classe r√©elle : **bleu pour 0**, **rouge pour 1**.

**Objectif :** entra√Æner un mod√®le capable de pr√©dire la probabilit√© qu‚Äôun point appartienne √† la classe 1.

---

# 2. R√©gression logistique sur les donn√©es

![R√©gression logistique et √©chantillons test](img/fig2_regression_et_points.png)

Nous utilisons une **r√©gression logistique** pour mod√©liser la probabilit√© d‚Äôappartenance √† la classe 1.  
La courbe noire repr√©sente la **fonction de pr√©diction** : plus un point est √† droite, plus sa probabilit√© d'√™tre de classe 1 augmente.

Les **points color√©s** montrent nos √©chantillons de test.  
Le **seuil de d√©cision** est fix√© √† **0.5** :  
- Si la probabilit√© pr√©dit plus que 0.5 ‚Üí classe 1.
- Sinon ‚Üí classe 0.

---

# 3. L'importance du choix du seuil

![Matrices de confusion pour diff√©rents seuils](img/matrices_confusion_seuils.png)

Selon le **seuil choisi**, le comportement du mod√®le change :

- **Seuil 0.3** : le mod√®le est tr√®s optimiste ‚Üí il pr√©dit plus souvent la classe 1.
- **Seuil 0.5** : √©quilibre par d√©faut.
- **Seuil 0.7** : le mod√®le devient plus conservateur ‚Üí il faut √™tre tr√®s s√ªr pour pr√©dire 1.

üëâ **Changer le seuil impacte directement le compromis entre faux positifs et faux n√©gatifs.**

---

# 4. Introduction √† la courbe ROC

Une **courbe ROC** (Receiver Operating Characteristic) repr√©sente **la performance du mod√®le pour tous les seuils possibles**.

![Courbe ROC](img/roc_curve.png)

Sur cette figure :
- L‚Äôaxe **x** est le **taux de faux positifs** (FPR).
- L‚Äôaxe **y** est le **taux de vrais positifs** (TPR).

Chaque point de la courbe correspond √† un **seuil de d√©cision diff√©rent** :
- Un mod√®le parfait aurait une courbe qui monte directement en haut √† gauche.
- Un mod√®le al√©atoire suit la diagonale (ligne pointill√©e).

**Astuces de lecture :**
- Plus la courbe est proche du coin sup√©rieur gauche, meilleur est le mod√®le.
- L‚Äô**AUC** (Area Under Curve) mesure la surface sous la courbe ROC : plus elle est proche de 1, mieux c‚Äôest.

---

# 5. Comment calcule-t-on une courbe ROC ?

La m√©thode est simple :
1. **Classer** les exemples par probabilit√© d√©croissante.
2. **Balayer tous les seuils possibles** (de 1 √† 0).
3. √Ä chaque seuil, calculer :
   - **TPR** (True Positive Rate) = Vrais positifs / (Vrais positifs + Faux n√©gatifs)
   - **FPR** (False Positive Rate) = Faux positifs / (Faux positifs + Vrais n√©gatifs)
4. Tracer **TPR** en fonction de **FPR**.

---

# 6. Comparaison de plusieurs mod√®les

![Comparaison des courbes ROC](img/comparaison_modeles_roc.png)

Sur cette figure, nous comparons :
- Notre mod√®le r√©el (courbe bleue)
- Un mod√®le parfait (courbe verte : AUC = 1)
- Un mod√®le al√©atoire (courbe orange : AUC ‚âà 0.5)

**On observe que** :
- Notre mod√®le est **meilleur qu‚Äôun tirage al√©atoire**.
- Il est cependant **loin de la perfection** (ce qui est normal sur des donn√©es bruit√©es).

---

# 7. Bonus : Courbe Precision-Recall

![Courbe Precision-Recall](img/precision_recall_curve.png)

Une autre mani√®re d‚Äô√©valuer les performances est d‚Äôutiliser la **courbe Pr√©cision-Rappel**, particuli√®rement utile si :

- Les classes sont **d√©s√©quilibr√©es**.
- On veut maximiser la **pr√©cision** ou le **rappel** selon l‚Äôapplication.

---

# üìà R√©sum√© : Pourquoi utiliser une courbe ROC ?

- Elle permet d‚Äô√©valuer **l'impact du seuil** sans en choisir un arbitrairement.
- Elle montre **l'√©quilibre entre sensibilit√© et sp√©cificit√©**.
- Elle est **ind√©pendante du seuil choisi**.
- L'**AUC** donne un indicateur synth√©tique de qualit√©.

üëâ La courbe ROC est un outil **fondamental** pour comparer objectivement des mod√®les de classification binaire.

---

# ‚úÖ Conclusion

En utilisant un dataset simple et des visualisations progressives, nous avons vu :
- Comment un mod√®le de classification produit des probabilit√©s.
- Comment le seuil de d√©cision impacte les erreurs.
- Comment la courbe ROC repr√©sente la performance globale du mod√®le.

**La courbe ROC est un alli√© pr√©cieux pour comprendre, ajuster, et comparer nos mod√®les !**
